{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import (absolute_import, division, print_function,\n",
    "                        unicode_literals)\n",
    "\n",
    "import datetime  # For datetime objects\n",
    "import os.path  # To manage paths\n",
    "import sys  # To find out the script name (in argv[0])\n",
    "import math\n",
    "\n",
    "# Import the backtrader platform\n",
    "import backtrader as bt\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBR\n",
    "\n",
    "size = [1,5,10,20]\n",
    "lag = [22,63,126,252]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Stratey\n",
    "class GBRStrategy(bt.Strategy):\n",
    "    \n",
    "    \n",
    "    def log(self, txt, dt=None):\n",
    "        ''' Logging function for this strategy'''\n",
    "        dt = dt or self.datas[0].datetime.date(0)\n",
    "        print('%s, %s' % (dt.isoformat(), txt))\n",
    "\n",
    "    def __init__(self):\n",
    "        # Keep a reference to the \"close\" line in the data[0] dataseries\n",
    "        self.dataclose = self.datas[0].close\n",
    "        self.dataclose_spx = self.datas[1].close\n",
    "\n",
    "        \n",
    "        # Add indicators\n",
    "        self.sma_short = bt.indicators.SimpleMovingAverage(self.datas[0], period=50)\n",
    "        self.sma_long = bt.indicators.SimpleMovingAverage(self.datas[0], period=200)\n",
    "        \n",
    "        # Add X variables\n",
    "        self.dsma = self.sma_short - self.sma_long\n",
    "        self.mom = bt.Cmp(self.sma_short, self.sma_long)\n",
    "        \n",
    "        self.rsi_overbought = bt.indicators.RSI(self.datas[0]) > 70\n",
    "        self.rsi_oversold = bt.indicators.RSI(self.datas[0]) <30\n",
    "        \n",
    "        self.cross_up = bt.indicators.CrossUp(self.sma_short, self.sma_long) \n",
    "        self.cross_down = bt.indicators.CrossDown(self.sma_short, self.sma_long)\n",
    "        \n",
    "        # Compute daily return\n",
    "        self.vix_returns = self.dataclose / self.dataclose(-1) - 1\n",
    "        \n",
    "\n",
    "    def next(self):\n",
    "        \n",
    "        lag = 1\n",
    "        # Set the length of training set\n",
    "        trainsize = 63\n",
    "        count = len(self.vix_returns) - 200\n",
    "        \n",
    "        # Simply log the closing price of the series from the reference\n",
    "\n",
    "        #self.log('VIX Return, %.4f' % self.vix_returns[0])\n",
    "        \n",
    "        # Regression\n",
    "        \n",
    "        trainX1 = pd.DataFrame(self.mom.get(ago=-lag, size=trainsize).tolist())\n",
    "        trainX2 = pd.DataFrame(self.rsi_overbought.get(ago=-lag, size=trainsize).tolist())\n",
    "        trainX3 = pd.DataFrame(self.rsi_oversold.get(ago=-lag, size=trainsize).tolist())\n",
    "        trainX4 = pd.DataFrame(self.cross_up.get(ago=-lag, size=trainsize).tolist())\n",
    "        trainX5 = pd.DataFrame(self.cross_down.get(ago=-lag, size=trainsize).tolist())\n",
    "        \n",
    "        trainX = pd.concat([trainX1, trainX2, trainX3, trainX4, trainX5], axis=1)\n",
    "        \n",
    "        trainY = np.array(self.vix_returns.get(ago=0, size=trainsize).tolist()).reshape(-1, 1)\n",
    "        \n",
    "        testX = np.array([self.mom[1-lag], self.rsi_overbought[1-lag], self.rsi_oversold[1-lag], self.cross_up[1-lag], self.cross_down[1-lag]]).reshape(-1,5)\n",
    "        testY = self.vix_returns[0]\n",
    "        \n",
    "        mse = []\n",
    "        r_pred_gb = []\n",
    "        y_pred = []\n",
    "        \n",
    "        \n",
    "        if count <= trainsize + lag - 1:\n",
    "            \n",
    "            # Set the predicted return to 0 if no regression is running\n",
    "            r_pred_gb.append(0)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            gb_regr = GradientBoostingRegressor()\n",
    "            gb_regr.fit(trainX, trainY.ravel())\n",
    "            tmp_pred = gb_regr.predict(testX)\n",
    "            y_pred = gb_regr.predict(trainX)\n",
    "            \n",
    "            r_pred_gb.append(tmp_pred) # use current data to predict\n",
    "            mse.append(math.pow((tmp_pred-testY), 2))\n",
    "           # print('%.4f' % r2_score(trainY,y_pred))\n",
    "            print('%.4f' % mse[-1])\n",
    "            \n",
    "            \n",
    "        #print('Predicted return, %.4f' % r_pred_gb[-1])\n",
    "        \n",
    "         \n",
    "        if r_pred_gb[-1] > 0:\n",
    "\n",
    "                # BUY, BUY, BUY!!! (with all possible default parameters)\n",
    "           # self.log('BUY CREATE, %.2f' % self.dataclose[0])\n",
    "\n",
    "                # Keep track of the created order to avoid a 2nd order\n",
    "            self.order = self.buy()\n",
    "\n",
    "        if r_pred_gb[-1] < 0:\n",
    "                # SELL, SELL, SELL!!! (with all possible default parameters)\n",
    "           # self.log('SELL CREATE, %.2f' % self.dataclose[0])\n",
    "\n",
    "                # Keep track of the created order to avoid a 2nd order\n",
    "            self.order = self.sell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005\n",
      "0.0002\n",
      "0.0170\n",
      "0.0163\n",
      "0.0005\n",
      "0.0002\n",
      "0.0002\n",
      "0.0000\n",
      "0.0011\n",
      "0.0004\n",
      "0.0161\n",
      "0.0093\n",
      "0.0134\n",
      "0.0037\n",
      "0.0024\n",
      "0.0009\n",
      "0.0016\n",
      "0.0002\n",
      "0.0037\n",
      "0.0051\n",
      "0.0002\n",
      "0.0001\n",
      "0.0008\n",
      "0.0021\n",
      "0.0001\n",
      "0.0009\n",
      "0.0026\n",
      "0.0139\n",
      "0.0016\n",
      "0.0043\n",
      "0.0229\n",
      "0.0050\n",
      "0.0028\n",
      "0.0009\n",
      "0.0076\n",
      "0.0027\n",
      "0.0000\n",
      "0.0000\n",
      "0.0001\n",
      "0.0001\n",
      "0.0035\n",
      "0.0000\n",
      "0.0002\n",
      "0.0003\n",
      "0.0001\n",
      "0.0004\n",
      "0.0011\n",
      "0.0017\n",
      "0.0001\n",
      "0.0001\n",
      "0.0016\n",
      "0.0025\n",
      "0.0000\n",
      "0.0000\n",
      "0.0063\n",
      "0.0030\n",
      "0.0000\n",
      "0.0018\n",
      "0.0061\n",
      "0.0002\n",
      "0.0026\n",
      "0.0003\n",
      "0.0003\n",
      "0.0000\n",
      "0.0001\n",
      "0.0006\n",
      "0.0008\n",
      "0.0004\n",
      "0.0008\n",
      "0.0055\n",
      "0.0020\n",
      "0.0000\n",
      "0.0037\n",
      "0.0071\n",
      "0.0008\n",
      "0.0016\n",
      "0.0019\n",
      "0.0128\n",
      "0.0001\n",
      "0.0007\n",
      "0.0003\n",
      "0.0114\n",
      "0.0017\n",
      "0.0000\n",
      "0.0008\n",
      "0.0009\n",
      "0.0011\n",
      "0.0006\n",
      "0.0017\n",
      "0.0102\n",
      "0.0035\n",
      "0.0006\n",
      "0.0065\n",
      "0.0016\n",
      "0.0004\n",
      "0.0002\n",
      "0.0060\n",
      "0.1012\n",
      "0.0292\n",
      "0.0037\n",
      "0.0020\n",
      "0.0034\n",
      "0.0008\n",
      "0.0050\n",
      "0.0001\n",
      "0.0031\n",
      "0.0000\n",
      "0.0702\n",
      "0.0000\n",
      "0.0136\n",
      "0.0119\n",
      "0.0013\n",
      "0.0001\n",
      "0.0035\n",
      "0.0106\n",
      "0.0001\n",
      "0.0082\n",
      "0.0015\n",
      "0.0030\n",
      "0.0043\n",
      "0.0001\n",
      "0.0014\n",
      "0.0000\n",
      "0.0008\n",
      "0.0003\n",
      "0.0001\n",
      "0.0001\n",
      "0.0004\n",
      "0.0000\n",
      "0.0001\n",
      "0.0000\n",
      "0.0003\n",
      "0.0023\n",
      "0.0018\n",
      "0.0036\n",
      "0.0025\n",
      "0.0001\n",
      "0.0013\n",
      "0.0032\n",
      "0.0104\n",
      "0.0001\n",
      "0.0029\n",
      "0.0000\n",
      "0.0154\n",
      "0.0071\n",
      "0.0135\n",
      "0.0292\n",
      "0.0034\n",
      "0.0046\n",
      "0.0001\n",
      "0.0002\n",
      "0.0019\n",
      "0.0035\n",
      "0.0000\n",
      "0.0006\n",
      "0.0194\n",
      "0.0281\n",
      "0.0022\n",
      "0.0039\n",
      "0.0299\n",
      "0.0338\n",
      "0.0013\n",
      "0.0515\n",
      "0.0498\n",
      "0.0321\n",
      "0.0035\n",
      "0.0133\n",
      "0.0036\n",
      "0.0013\n",
      "0.0156\n",
      "0.0008\n",
      "0.0038\n",
      "0.0026\n",
      "0.0010\n",
      "0.0001\n",
      "0.0040\n",
      "0.0023\n",
      "0.0025\n",
      "0.0018\n",
      "0.0001\n",
      "0.0000\n",
      "0.0024\n",
      "0.0019\n",
      "0.0016\n",
      "0.0004\n",
      "0.0000\n",
      "0.0013\n",
      "0.0032\n",
      "0.0008\n",
      "0.0012\n",
      "0.0004\n",
      "0.0095\n",
      "0.0029\n",
      "0.0097\n",
      "0.0010\n",
      "0.0002\n",
      "0.0019\n",
      "0.0350\n",
      "0.0050\n",
      "0.0372\n",
      "0.0231\n",
      "0.0079\n",
      "0.0022\n",
      "0.0367\n",
      "0.0377\n",
      "0.0228\n",
      "0.0011\n",
      "0.0078\n",
      "0.0017\n",
      "0.0016\n",
      "0.0000\n",
      "0.0007\n",
      "0.0020\n",
      "0.0360\n",
      "0.0077\n",
      "0.0107\n",
      "0.0019\n",
      "0.0097\n",
      "0.0174\n",
      "0.0005\n",
      "0.0116\n",
      "0.0018\n",
      "0.0014\n",
      "0.0013\n",
      "0.0049\n",
      "0.0030\n",
      "0.0034\n",
      "0.0185\n",
      "0.0001\n",
      "0.0053\n",
      "0.0105\n",
      "0.0313\n",
      "0.0086\n",
      "0.0113\n",
      "0.0071\n",
      "0.0139\n",
      "0.0023\n",
      "0.0079\n",
      "0.0003\n",
      "0.0038\n",
      "0.0067\n",
      "0.0007\n",
      "0.0110\n",
      "0.0026\n",
      "0.0045\n",
      "0.0010\n",
      "0.0003\n",
      "0.0052\n",
      "0.0001\n",
      "0.0044\n",
      "0.0000\n",
      "0.0000\n",
      "0.0024\n",
      "0.0009\n",
      "0.0031\n",
      "0.0004\n",
      "0.0004\n",
      "0.0053\n",
      "0.0004\n",
      "0.0092\n",
      "0.0000\n",
      "0.0085\n",
      "0.0012\n",
      "0.0007\n",
      "0.0000\n",
      "0.0116\n",
      "0.0001\n",
      "0.0050\n",
      "0.0010\n",
      "0.0003\n",
      "0.0171\n",
      "0.0004\n",
      "0.0025\n",
      "0.0016\n",
      "0.0026\n",
      "0.0002\n",
      "0.0007\n",
      "0.0000\n",
      "0.0000\n",
      "0.0025\n",
      "0.0036\n",
      "0.0013\n",
      "0.0120\n",
      "0.0002\n",
      "0.0013\n",
      "0.0000\n",
      "0.0037\n",
      "0.0032\n",
      "0.0002\n",
      "0.0016\n",
      "0.0002\n",
      "0.0001\n",
      "0.0041\n",
      "0.0027\n",
      "0.0054\n",
      "0.0054\n",
      "0.0165\n",
      "0.0001\n",
      "0.0107\n",
      "0.0021\n",
      "0.0002\n",
      "0.0234\n",
      "0.0049\n",
      "0.0000\n",
      "0.0002\n",
      "0.0058\n",
      "0.0008\n",
      "0.0007\n",
      "0.0001\n",
      "0.0000\n",
      "0.0036\n",
      "0.0000\n",
      "0.0236\n",
      "0.0036\n",
      "0.0000\n",
      "0.0012\n",
      "0.0000\n",
      "0.0002\n",
      "0.0020\n",
      "0.0050\n",
      "0.0015\n",
      "0.0048\n",
      "0.0035\n",
      "0.0080\n",
      "0.0009\n",
      "0.0047\n",
      "0.0122\n",
      "0.0019\n",
      "0.0007\n",
      "0.0087\n",
      "0.0030\n",
      "0.0080\n",
      "0.0025\n",
      "0.0085\n",
      "0.0028\n",
      "0.0000\n",
      "0.1116\n",
      "0.0018\n",
      "0.0155\n",
      "0.0013\n",
      "0.0000\n",
      "0.0037\n",
      "0.0446\n",
      "0.0000\n",
      "0.0273\n",
      "0.0320\n",
      "0.0018\n",
      "0.0002\n",
      "0.0077\n",
      "0.0002\n",
      "0.0005\n",
      "0.0000\n",
      "0.0001\n",
      "0.0015\n",
      "0.0067\n",
      "0.0166\n",
      "0.0206\n",
      "0.0053\n",
      "0.0009\n",
      "0.0000\n",
      "0.0011\n",
      "0.0011\n",
      "0.0015\n",
      "0.0097\n",
      "0.0010\n",
      "0.0078\n",
      "0.0139\n",
      "0.0001\n",
      "0.0002\n",
      "0.0028\n",
      "0.0001\n",
      "0.0030\n",
      "0.0099\n",
      "0.0595\n",
      "0.1988\n",
      "0.0000\n",
      "0.0810\n",
      "0.0016\n",
      "0.0000\n",
      "0.0047\n",
      "0.0117\n",
      "0.0082\n",
      "0.0214\n",
      "0.0000\n",
      "0.0085\n",
      "0.0073\n",
      "0.0041\n",
      "0.0029\n",
      "0.0008\n",
      "0.0035\n",
      "0.0027\n",
      "0.0010\n",
      "0.0001\n",
      "0.0048\n",
      "0.0057\n",
      "0.0162\n",
      "0.0000\n",
      "0.0049\n",
      "0.0002\n",
      "0.0290\n",
      "0.0007\n",
      "0.0067\n",
      "0.0051\n",
      "0.0037\n",
      "0.0029\n",
      "0.0000\n",
      "0.0014\n",
      "0.0014\n",
      "0.0000\n",
      "0.0014\n",
      "0.0112\n",
      "0.0011\n",
      "0.0090\n",
      "0.0021\n",
      "0.0001\n",
      "0.0043\n",
      "0.0053\n",
      "0.0142\n",
      "0.0002\n",
      "0.0050\n",
      "0.0005\n",
      "0.0033\n",
      "0.0011\n",
      "0.0019\n",
      "0.0023\n",
      "0.0016\n",
      "0.0060\n",
      "0.0003\n",
      "0.0013\n",
      "0.0261\n",
      "0.0041\n",
      "0.0035\n",
      "0.0225\n",
      "0.0095\n",
      "0.0080\n",
      "0.0018\n",
      "0.0097\n",
      "0.0002\n",
      "0.0066\n",
      "0.0003\n",
      "0.0007\n",
      "0.0016\n",
      "0.0000\n",
      "0.0053\n",
      "0.0066\n",
      "0.0081\n",
      "0.0199\n",
      "0.0307\n",
      "0.0055\n",
      "0.0131\n",
      "0.0132\n",
      "0.0002\n",
      "0.0664\n",
      "0.0051\n",
      "0.0066\n",
      "0.0220\n",
      "0.0035\n",
      "0.0079\n",
      "0.0095\n",
      "0.0124\n",
      "0.0035\n",
      "0.0002\n",
      "0.0062\n",
      "0.0020\n",
      "0.0060\n",
      "0.0028\n",
      "0.0178\n",
      "0.0048\n",
      "0.0036\n",
      "0.0420\n",
      "0.0049\n",
      "0.0121\n",
      "0.0071\n",
      "0.0127\n",
      "0.0035\n",
      "0.0134\n",
      "0.0024\n",
      "0.0021\n",
      "0.0020\n",
      "0.0295\n",
      "0.0047\n",
      "0.0063\n",
      "0.0003\n",
      "0.0016\n",
      "0.0118\n",
      "0.0004\n",
      "0.0081\n",
      "0.0007\n",
      "0.0000\n",
      "0.0036\n",
      "0.0098\n",
      "0.0000\n",
      "0.0004\n",
      "0.0032\n",
      "0.0119\n",
      "0.0035\n",
      "0.0065\n",
      "0.0014\n",
      "0.0032\n",
      "0.0039\n",
      "0.0057\n",
      "0.0004\n",
      "0.0072\n",
      "0.0009\n",
      "0.0008\n",
      "0.0211\n",
      "0.0016\n",
      "0.0008\n",
      "0.0000\n",
      "0.0007\n",
      "0.0047\n",
      "0.0006\n",
      "0.0004\n",
      "0.0075\n",
      "0.0006\n",
      "0.0000\n",
      "0.0113\n",
      "0.0011\n",
      "0.0008\n",
      "0.0002\n",
      "0.0009\n",
      "0.0030\n",
      "0.0002\n",
      "0.0010\n",
      "0.0088\n",
      "0.0003\n",
      "0.0008\n",
      "0.0035\n",
      "0.0063\n",
      "0.0089\n",
      "0.0070\n",
      "0.0219\n",
      "0.0020\n",
      "0.0014\n",
      "0.0000\n",
      "0.0001\n",
      "0.0021\n",
      "0.0013\n",
      "0.0003\n",
      "0.0006\n",
      "0.0010\n",
      "0.0047\n",
      "0.0009\n",
      "0.0061\n",
      "0.0000\n",
      "0.0000\n",
      "0.0119\n",
      "0.0011\n",
      "0.0035\n",
      "0.0041\n",
      "0.0008\n",
      "0.0001\n",
      "0.0050\n",
      "0.0000\n",
      "0.0033\n",
      "0.0066\n",
      "0.0002\n",
      "0.0021\n",
      "0.0004\n",
      "0.0037\n",
      "0.0006\n",
      "0.0005\n",
      "0.0046\n",
      "0.0016\n",
      "0.0074\n",
      "0.0011\n",
      "0.0009\n",
      "0.0003\n",
      "0.0071\n",
      "0.0000\n",
      "0.0013\n",
      "0.0001\n",
      "0.0003\n",
      "0.0010\n",
      "0.0000\n",
      "0.0017\n",
      "0.0259\n",
      "0.0500\n",
      "0.0000\n",
      "0.0000\n",
      "0.0021\n",
      "0.0000\n",
      "0.0036\n",
      "0.0000\n",
      "0.0186\n",
      "0.0362\n",
      "0.2289\n",
      "0.0076\n",
      "0.0496\n",
      "0.0142\n",
      "0.0044\n",
      "0.0035\n",
      "0.0025\n",
      "0.0019\n",
      "0.0003\n",
      "0.0116\n",
      "0.0005\n",
      "0.0000\n",
      "0.0018\n",
      "0.0005\n",
      "0.0003\n",
      "0.0005\n",
      "0.0017\n",
      "0.0004\n",
      "0.0061\n",
      "0.0037\n",
      "0.0044\n",
      "0.0001\n",
      "0.0005\n",
      "0.0001\n",
      "0.0046\n",
      "0.0021\n",
      "0.0052\n",
      "0.0016\n",
      "0.0013\n",
      "0.0071\n",
      "0.0001\n",
      "0.0001\n",
      "0.0010\n",
      "0.0011\n",
      "0.0001\n",
      "0.0004\n",
      "0.0047\n",
      "0.0013\n",
      "0.0038\n",
      "0.0001\n",
      "0.0065\n",
      "0.0000\n",
      "0.0066\n",
      "0.0001\n",
      "0.0000\n",
      "0.0031\n",
      "0.0001\n",
      "0.0003\n",
      "0.0000\n",
      "0.0131\n",
      "0.0000\n",
      "0.0001\n",
      "0.0020\n",
      "0.1806\n",
      "0.0187\n",
      "0.0294\n",
      "0.0001\n",
      "0.0112\n",
      "0.0037\n",
      "0.0000\n",
      "0.0004\n",
      "0.0270\n",
      "0.0095\n",
      "0.0008\n",
      "0.0332\n",
      "0.0092\n",
      "0.0030\n",
      "0.0164\n",
      "0.0031\n",
      "0.0003\n",
      "0.0000\n",
      "0.0025\n",
      "0.0003\n",
      "0.0020\n",
      "0.0002\n",
      "0.0196\n",
      "0.0007\n",
      "0.0015\n",
      "0.0019\n",
      "0.0000\n",
      "0.0045\n",
      "0.0041\n",
      "0.0028\n",
      "0.0013\n",
      "0.0009\n",
      "0.0008\n",
      "0.0026\n",
      "0.0048\n",
      "0.0020\n",
      "0.0020\n",
      "0.0059\n",
      "0.0008\n",
      "0.0765\n",
      "0.0059\n",
      "0.0343\n",
      "0.0002\n",
      "0.0601\n",
      "0.0001\n",
      "0.0026\n",
      "0.0001\n",
      "0.0076\n",
      "0.0002\n",
      "0.0015\n",
      "0.0022\n",
      "0.0018\n",
      "0.0001\n",
      "0.0000\n",
      "0.0002\n",
      "0.0032\n",
      "0.0008\n",
      "0.0006\n",
      "0.0021\n",
      "0.0001\n",
      "0.0223\n",
      "0.0014\n",
      "0.0008\n",
      "0.0010\n",
      "0.0052\n",
      "0.0058\n",
      "0.0000\n",
      "0.0012\n",
      "0.0011\n",
      "0.0023\n",
      "0.0016\n",
      "0.0006\n",
      "0.0004\n",
      "0.0001\n",
      "0.0000\n",
      "0.0020\n",
      "0.0056\n",
      "0.0008\n",
      "0.0020\n",
      "0.0078\n",
      "0.0064\n",
      "0.0003\n",
      "0.0011\n",
      "0.0004\n",
      "0.0001\n",
      "0.0004\n",
      "0.0007\n",
      "0.0006\n",
      "0.0033\n",
      "0.0026\n",
      "0.0005\n",
      "0.0096\n",
      "0.0003\n",
      "0.0037\n",
      "0.0006\n",
      "0.0003\n",
      "0.0000\n",
      "0.0150\n",
      "0.0001\n",
      "0.0002\n",
      "0.0002\n",
      "0.0057\n",
      "0.0018\n",
      "0.0000\n",
      "0.0004\n",
      "0.0018\n",
      "0.0000\n",
      "0.0006\n",
      "0.0007\n",
      "0.0135\n",
      "0.0003\n",
      "0.0005\n",
      "0.0001\n",
      "0.0002\n",
      "0.0000\n",
      "0.0004\n",
      "0.0028\n",
      "0.0045\n",
      "0.0009\n",
      "0.0034\n",
      "0.0049\n",
      "0.0008\n",
      "0.0004\n",
      "0.0012\n",
      "0.0013\n",
      "0.0027\n",
      "0.0007\n",
      "0.0067\n",
      "0.0029\n",
      "0.0012\n",
      "0.0001\n",
      "0.0000\n",
      "0.0096\n",
      "0.0006\n",
      "0.0004\n",
      "0.0002\n",
      "0.0014\n",
      "0.0062\n",
      "0.0001\n",
      "0.0001\n",
      "0.0052\n",
      "0.0000\n",
      "0.0022\n",
      "0.0083\n",
      "0.0017\n",
      "0.0013\n",
      "0.0076\n",
      "0.0045\n",
      "0.0016\n",
      "0.0000\n",
      "0.0078\n",
      "0.0005\n",
      "0.0009\n",
      "0.0032\n",
      "0.0008\n",
      "0.0680\n",
      "0.0001\n",
      "0.0000\n",
      "0.0023\n",
      "0.0017\n",
      "0.0043\n",
      "0.0022\n",
      "0.0001\n",
      "0.0005\n",
      "0.0001\n",
      "0.0058\n",
      "0.0003\n",
      "0.0006\n",
      "0.0013\n",
      "0.0004\n",
      "0.0000\n",
      "0.0004\n",
      "0.2486\n",
      "0.0047\n",
      "0.0342\n",
      "0.0094\n",
      "0.0005\n",
      "0.0047\n",
      "0.0000\n",
      "0.0004\n",
      "0.0031\n",
      "0.0000\n",
      "0.0028\n",
      "0.0003\n",
      "0.0008\n",
      "0.0011\n",
      "0.0001\n",
      "0.0006\n",
      "0.0024\n",
      "0.0043\n",
      "0.0088\n",
      "0.0003\n",
      "0.0004\n",
      "0.0027\n",
      "0.0000\n",
      "0.0019\n",
      "0.0002\n",
      "0.0007\n",
      "0.0020\n",
      "0.0002\n",
      "0.0126\n",
      "0.0093\n",
      "0.0182\n",
      "0.0007\n",
      "0.0000\n",
      "0.0003\n",
      "0.0162\n",
      "0.0125\n",
      "0.0001\n",
      "0.0005\n",
      "0.0029\n",
      "0.0014\n",
      "0.0014\n",
      "0.0012\n",
      "0.0001\n",
      "0.0001\n",
      "0.0004\n",
      "0.0004\n",
      "0.0000\n",
      "0.0000\n",
      "0.0003\n",
      "0.0025\n",
      "0.0002\n",
      "0.0000\n",
      "0.0004\n",
      "0.0002\n",
      "0.0001\n",
      "0.0018\n",
      "0.0002\n",
      "0.0097\n",
      "0.0001\n",
      "0.2534\n",
      "0.0002\n",
      "0.0473\n",
      "0.0012\n",
      "0.0008\n",
      "0.0997\n",
      "0.0087\n",
      "0.0073\n",
      "0.0219\n",
      "0.0047\n",
      "0.0002\n",
      "0.0077\n",
      "0.0000\n",
      "0.0006\n",
      "0.0026\n",
      "0.0043\n",
      "0.0026\n",
      "0.0387\n",
      "0.0035\n",
      "0.0003\n",
      "0.0015\n",
      "0.0148\n",
      "0.0005\n",
      "0.0002\n",
      "0.0002\n",
      "0.0011\n",
      "0.0001\n",
      "0.0000\n",
      "0.0021\n",
      "0.0003\n",
      "0.0002\n",
      "0.0031\n",
      "0.0001\n",
      "0.0014\n",
      "0.0014\n",
      "0.0001\n",
      "0.0001\n",
      "0.0000\n",
      "0.0001\n",
      "0.0025\n",
      "0.0020\n",
      "0.0041\n",
      "0.0010\n",
      "0.0009\n",
      "0.0000\n",
      "0.0014\n",
      "0.0006\n",
      "0.0010\n",
      "0.0010\n",
      "0.0001\n",
      "0.0003\n",
      "0.0100\n",
      "0.0000\n",
      "0.0000\n",
      "0.0000\n",
      "0.0195\n",
      "0.0040\n",
      "0.0015\n",
      "0.0000\n",
      "0.0012\n",
      "0.0074\n",
      "0.0005\n",
      "0.0021\n",
      "0.0001\n",
      "0.0055\n",
      "0.0055\n",
      "0.0003\n",
      "0.0000\n",
      "0.0179\n",
      "0.0110\n",
      "0.0008\n",
      "0.0050\n",
      "0.0074\n",
      "0.0002\n",
      "0.0005\n",
      "0.0004\n",
      "0.0002\n",
      "0.0041\n",
      "0.0025\n",
      "0.0001\n",
      "0.0004\n",
      "0.0011\n",
      "0.0009\n",
      "0.0061\n",
      "0.0034\n",
      "0.0007\n",
      "0.0036\n",
      "0.0006\n",
      "0.0007\n",
      "0.0108\n",
      "0.0001\n",
      "0.0024\n",
      "0.0012\n",
      "0.0002\n",
      "0.0007\n",
      "0.0010\n",
      "0.0003\n",
      "0.0010\n",
      "0.0062\n",
      "0.0141\n",
      "0.0044\n",
      "0.0000\n",
      "0.0000\n",
      "0.0009\n",
      "0.0031\n",
      "0.0008\n",
      "0.0000\n",
      "0.0006\n",
      "0.0199\n",
      "0.0002\n",
      "0.0004\n",
      "0.0069\n",
      "0.0007\n",
      "0.0000\n",
      "0.0009\n",
      "0.0000\n",
      "0.0021\n",
      "0.0586\n",
      "0.0299\n",
      "0.0088\n",
      "0.0002\n",
      "0.1440\n",
      "0.2955\n",
      "0.0000\n",
      "0.0084\n",
      "0.1173\n",
      "0.1143\n",
      "Final Portfolio Value: 105764.52\n",
      "Sharpe Ratio: OrderedDict([('sharperatio', -0.01039425824118294)])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Create a cerebro entity\n",
    "    cerebro = bt.Cerebro()\n",
    "\n",
    "    # Add a strategy\n",
    "    cerebro.addstrategy(GBRStrategy)\n",
    "    cerebro.addanalyzer(bt.analyzers.SharpeRatio, _name='mySharpe')\n",
    "\n",
    "    datapath_spx = os.path.join('../../../datas/spx-2013-2018.txt')\n",
    "    datapath_vix= os.path.join('../../../datas/vix-2013-2018.txt')\n",
    "\n",
    "    # Create a Data Feed\n",
    "    data_vix = bt.feeds.YahooFinanceCSVData(\n",
    "        dataname=datapath_vix,\n",
    "        # Do not pass values before this date\n",
    "        #fromdate=datetime.datetime(2018, 1, 1),\n",
    "        # Do not pass values before this date\n",
    "        #todate=datetime.datetime(2018, 2, 9),\n",
    "        # Do not pass values after this date\n",
    "        reverse=False)\n",
    "\n",
    "    data_spx = bt.feeds.YahooFinanceCSVData(\n",
    "        dataname=datapath_spx,\n",
    "        # Do not pass values before this date\n",
    "        #fromdate=datetime.datetime(2018, 1, 1),\n",
    "        # Do not pass values before this date\n",
    "        #todate=datetime.datetime(2018, 2, 9),\n",
    "        # Do not pass values after this date\n",
    "        reverse=False)\n",
    "\n",
    "    \n",
    "    # Add the Data Feed to Cerebro\n",
    "    cerebro.adddata(data_vix)\n",
    "    cerebro.adddata(data_spx)\n",
    "\n",
    "    # Set our desired cash start\n",
    "    cerebro.broker.setcash(100000.0)\n",
    "\n",
    "    # Write output\n",
    "    cerebro.addwriter(bt.WriterFile, out='gbr_252_1.csv',csv=True)\n",
    "    \n",
    "    # Print out the starting conditions\n",
    "    #print('Starting Portfolio Value: %.2f' % cerebro.broker.getvalue())\n",
    "\n",
    "    # Run over everything\n",
    "    thestrats = cerebro.run()\n",
    "    thestrat = thestrats[0]\n",
    "\n",
    "    # Print out the final result\n",
    "    print('Final Portfolio Value: %.2f' % cerebro.broker.getvalue())\n",
    "    #print('Holding Period Return:', thestrat.analyzers.myHPY.get_analysis())\n",
    "    print('Sharpe Ratio:', thestrat.analyzers.mySharpe.get_analysis())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Regressor (Lag = 1):\n",
    "\n",
    "Mean Squared Error, 0.1143\n",
    "\n",
    "Final Portfolio Value: 109367.76\n",
    "\n",
    "\n",
    "Lag = 5:\n",
    "\n",
    "Mean Squared Error, 0.0051\n",
    "\n",
    "Final Portfolio Value: 107887.95\n",
    "\n",
    "\n",
    "Lag = 10:\n",
    "\n",
    "Mean Squared Error, 0.0202\n",
    "\n",
    "Final Portfolio Value: 105910.57\n",
    "\n",
    "\n",
    "Lag = 20:\n",
    "\n",
    "Mean Squared Error, 0.0201\n",
    "\n",
    "Final Portfolio Value: 107116.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
